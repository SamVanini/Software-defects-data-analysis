{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca8ef36",
   "metadata": {},
   "source": [
    "# Software Defects Analysis & Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394ceeb",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e32372",
   "metadata": {},
   "source": [
    "## Step 2: Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fae10e",
   "metadata": {},
   "source": [
    "## Step 3: Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b8f06",
   "metadata": {},
   "source": [
    "## Step 4: Defects prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef5a45",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression\n",
    "\n",
    "#### The Core Idea\n",
    "\n",
    "Logistic Regression models the **probability** that an instance belongs to class 1 using the **logistic (sigmoid) function**.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "##### 1. Linear Combination\n",
    "\n",
    "First, compute a linear combination of features:\n",
    "\n",
    "```\n",
    "z = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n",
    "z = β₀ + Σ(βᵢ * xᵢ)\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `x₁, x₂, ..., xₙ` are the features\n",
    "- `β₀, β₁, ..., βₙ` are the coefficients (weights) to be learned\n",
    "- `β₀` is the intercept (bias)\n",
    "\n",
    "##### 2. Sigmoid Function\n",
    "\n",
    "Transform the linear output to a probability using the **sigmoid function**:\n",
    "\n",
    "```\n",
    "P(y=1|x) = σ(z) = 1 / (1 + e^(-z))\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `P(y=1|x)` is the probability that y=1 given features x\n",
    "- `e` is Euler's number (≈2.718)\n",
    "- The output is always between 0 and 1\n",
    "\n",
    "**Why sigmoid?** It maps any real number to a range [0, 1]\n",
    "\n",
    "```\n",
    "z → -∞    ⟹  P → 0\n",
    "z = 0     ⟹  P = 0.5\n",
    "z → +∞    ⟹  P → 1\n",
    "```\n",
    "\n",
    "##### 3. Decision Rule\n",
    "\n",
    "```\n",
    "Predict class 1 if P(y=1|x) ≥ 0.5\n",
    "Predict class 0 if P(y=1|x) < 0.5\n",
    "```\n",
    "\n",
    "#### Loss Function: Log Loss (Cross-Entropy)\n",
    "\n",
    "The model learns by minimizing the **log loss**:\n",
    "\n",
    "```\n",
    "L(β) = -1/m * Σ[yᵢ * log(P(yᵢ=1|xᵢ)) + (1-yᵢ) * log(1-P(yᵢ=1|xᵢ))]\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `m` is the number of training examples\n",
    "- `yᵢ` is the actual label (0 or 1)\n",
    "- `P(yᵢ=1|xᵢ)` is the predicted probability\n",
    "\n",
    "**Intuition**:\n",
    "\n",
    "- If actual y=1 and we predict P=0.9, loss is small (-log(0.9) ≈ 0.1)\n",
    "- If actual y=1 and we predict P=0.1, loss is large (-log(0.1) ≈ 2.3)\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "To prevent overfitting, add a penalty term:\n",
    "\n",
    "**L2 Regularization (Ridge):**\n",
    "\n",
    "```\n",
    "L(β) = Log Loss + λ * Σ(βᵢ²)\n",
    "```\n",
    "\n",
    "**L1 Regularization (Lasso):**\n",
    "\n",
    "```\n",
    "L(β) = Log Loss + λ * Σ|βᵢ|\n",
    "```\n",
    "\n",
    "Where `λ` controls regularization strength (larger λ = more regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf24b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb94d0c5",
   "metadata": {},
   "source": [
    "### Model 2: Random Forest\n",
    "\n",
    "#### The Core Idea\n",
    "\n",
    "Build many **decision trees** on random subsets of data and features, then **average** their predictions.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "##### 1. Single Decision Tree\n",
    "\n",
    "A decision tree splits data recursively to maximize **information gain** or minimize **impurity**.\n",
    "\n",
    "**Gini Impurity** (measure of randomness):\n",
    "\n",
    "```\n",
    "Gini(node) = 1 - Σ(pᵢ²)\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `pᵢ` is the proportion of class i samples in the node\n",
    "- Perfect purity: Gini = 0 (all samples same class)\n",
    "- Maximum impurity: Gini = 0.5 (equal mix of classes)\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Node with 60 class-0 and 40 class-1 samples:\n",
    "p₀ = 60/100 = 0.6\n",
    "p₁ = 40/100 = 0.4\n",
    "Gini = 1 - (0.6² + 0.4²) = 1 - 0.52 = 0.48\n",
    "```\n",
    "\n",
    "**Information Gain** when splitting:\n",
    "\n",
    "```\n",
    "IG = Gini(parent) - Weighted_Average[Gini(left_child), Gini(right_child)]\n",
    "```\n",
    "\n",
    "The algorithm chooses splits that **maximize Information Gain**.\n",
    "\n",
    "##### 2. Bootstrap Aggregating (Bagging)\n",
    "\n",
    "Random Forest creates diversity through **bagging**:\n",
    "\n",
    "1. **Randomly sample** m instances from training data (with replacement)\n",
    "2. **Randomly select** k features at each split (typically k = √n_features)\n",
    "3. Build a decision tree on this subset\n",
    "4. Repeat for n_trees\n",
    "\n",
    "##### 3. Prediction by Voting\n",
    "\n",
    "For binary classification:\n",
    "\n",
    "```\n",
    "P(y=1|x) = 1/N * Σ[tree_i predicts 1]\n",
    "```\n",
    "\n",
    "Where N is the number of trees.\n",
    "\n",
    "**Final prediction:**\n",
    "\n",
    "```\n",
    "ŷ = 1 if P(y=1|x) ≥ 0.5, else 0\n",
    "```\n",
    "\n",
    "#### Why Does It Work?\n",
    "\n",
    "**Law of Large Numbers:** Average of many predictions is more stable than individual predictions.\n",
    "\n",
    "**Bias-Variance Tradeoff:**\n",
    "\n",
    "- Single tree: Low bias, high variance (overfits)\n",
    "- Random Forest: Low bias, low variance (averaging reduces variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a944f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "926a0258",
   "metadata": {},
   "source": [
    "### Model 3: Gradient Boosting\n",
    "\n",
    "#### The Core Idea\n",
    "\n",
    "Build trees **sequentially**, where each tree corrects the errors of the previous trees.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "##### 1. Additive Model\n",
    "\n",
    "The prediction is a **sum** of multiple weak learners:\n",
    "\n",
    "```\n",
    "F(x) = f₀(x) + η*f₁(x) + η*f₂(x) + ... + η*fₘ(x)\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `F(x)` is the final prediction\n",
    "- `f₀(x)` is the initial prediction (usually mean)\n",
    "- `fᵢ(x)` are decision trees (weak learners)\n",
    "- `η` is the learning rate (0 < η ≤ 1)\n",
    "- `m` is the number of trees\n",
    "\n",
    "##### 2. Sequential Learning\n",
    "\n",
    "At each iteration t:\n",
    "\n",
    "**Step 1: Calculate residuals (errors)**\n",
    "\n",
    "```\n",
    "rᵢ⁽ᵗ⁾ = yᵢ - F⁽ᵗ⁻¹⁾(xᵢ)\n",
    "```\n",
    "\n",
    "**Step 2: Fit a new tree to residuals**\n",
    "\n",
    "```\n",
    "fₜ(x) ← fit_tree(X, r⁽ᵗ⁾)\n",
    "```\n",
    "\n",
    "**Step 3: Update the model**\n",
    "\n",
    "```\n",
    "F⁽ᵗ⁾(x) = F⁽ᵗ⁻¹⁾(x) + η * fₜ(x)\n",
    "```\n",
    "\n",
    "#### 3. For Binary Classification\n",
    "\n",
    "Use **log-odds** instead of class labels:\n",
    "\n",
    "```\n",
    "F(x) = log(P(y=1|x) / P(y=0|x))\n",
    "```\n",
    "\n",
    "Convert to probability:\n",
    "\n",
    "```\n",
    "P(y=1|x) = 1 / (1 + e^(-F(x)))\n",
    "```\n",
    "\n",
    "##### 4. Gradient Descent Intuition\n",
    "\n",
    "Gradient Boosting minimizes loss by following the **negative gradient**:\n",
    "\n",
    "```\n",
    "fₜ(x) ≈ -∂L/∂F(x)\n",
    "```\n",
    "\n",
    "Where L is the loss function (e.g., log loss).\n",
    "\n",
    "**Why \"Gradient\" Boosting?** Each new tree fits the **negative gradient** of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a66b257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22a11220",
   "metadata": {},
   "source": [
    "### Model 4: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5080b7f",
   "metadata": {},
   "source": [
    "### Model 5: K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bafbd4",
   "metadata": {},
   "source": [
    "### Model 6: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec5a5a",
   "metadata": {},
   "source": [
    "### Model 7: Neural Network (MLPClassifier)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
